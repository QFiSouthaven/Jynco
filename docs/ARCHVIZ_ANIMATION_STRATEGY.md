# ArchViz Animation Strategy: Integrating Advanced 2D Animation Pipelines

**Document Version:** 1.0
**Created:** November 14, 2025
**Project:** Video Foundry (Jynco)
**Purpose:** Strategic roadmap for integrating AI-driven 2D animation capabilities for architectural visualization

---

## Executive Summary

This document analyzes the integration of advanced AI-driven 2D animation techniques into the Video Foundry platform, specifically targeting architectural visualization (ArchViz) workflows. Based on the "New Blueprint" architectural analysis, this strategy leverages Video Foundry's existing **modular, open-source-first architecture** to add cutting-edge animation capabilities while maintaining the platform's core strengths.

### Key Findings

‚úÖ **Strong Foundation**: Video Foundry already implements 80% of the recommended architectural patterns
üéØ **Strategic Opportunity**: Adding specialized ArchViz workflows positions the platform uniquely in the market
‚ö° **Quick Wins Available**: Existing infrastructure enables rapid feature deployment

---

## Current Alignment Analysis

### ‚úÖ Already Implemented (Core Architecture)

| Blueprint Principle | Video Foundry Implementation | Location |
|-------------------|----------------------------|----------|
| **ComfyUI as Foundation** | ComfyUI adapter with full API integration | `workers/ai_worker/adapters/comfyui.py` |
| **Modular, Open-Source First** | Pluggable adapter pattern, no vendor lock-in | `workers/ai_worker/adapters/factory.py` |
| **Artist-in-the-Loop** | Timeline-based keyframe editing | `frontend/src/features/timeline/` |
| **Workflow Library** | Database-backed workflow templates | `backend/models/workflow.py` |
| **Node-Based Backend** | ComfyUI JSON workflow support | `workflows/*.json` |
| **Timeline UI** | Visual segment editor with drag-drop | `TimelineEditor.tsx` |
| **Real-time Progress** | WebSocket updates for render jobs | `backend/api/render.py` |
| **Distributed Processing** | RabbitMQ-based worker scaling | `workers/ai_worker/worker.py` |

### üéØ Enhancement Opportunities

| Blueprint Feature | Current Gap | Strategic Value | Implementation Effort |
|------------------|-------------|-----------------|---------------------|
| **ToonCrafter Integration** | Not yet supported | High - enables keyframe interpolation | Medium |
| **ControlNet Workflows** | No structural consistency templates | Critical - preserves ArchViz geometry | Low |
| **IPAdapter Workflows** | No style transfer templates | High - stylized ArchViz | Low |
| **Prompt Scheduling** | Static prompts only | High - dynamic animations | Medium |
| **2.5D Parallax** | No depth-based motion | Medium - immersive sketches | Medium |
| **React Flow UI** | Basic timeline only | Medium - advanced users | High |
| **Blender Integration** | No direct export | High - production workflows | High |

---

## Strategic Implementation Roadmap

### Phase 1: Advanced Workflow Templates (Weeks 1-3)

**Objective**: Expand workflow library with ArchViz-specific templates matching the blueprint recommendations.

#### 1.1 Stylized ArchViz Walkthrough (Video-to-Video)

**Use Case**: Transform 3D clay renders into stylized 2D "flipbook" animations

**Required Workflow Nodes**:
```json
{
  "workflow_type": "video-to-video-archviz",
  "components": [
    "AnimateDiff (temporal consistency)",
    "ControlNet Lineart (preserve geometry)",
    "ControlNet Depth (maintain perspective)",
    "IPAdapter (apply artistic style)",
    "VideoHelperSuite (output)"
  ]
}
```

**Implementation**:
```bash
# New workflow file location
workflows/archviz-video-to-video-stylized.json

# Database entry
backend/migrations/add_archviz_workflows.py
```

**Model Requirements**:
- AnimateDiff motion module (already supported)
- ControlNet models: `control_sd15_lineart.pth`, `control_sd15_depth.pth`
- IPAdapter model: `ip-adapter_sd15.safetensors`
- Base model: SDXL or SD 1.5

**API Integration** (already compatible):
```python
# No code changes needed - use existing workflow API
workflow_id = await adapter.initiate_generation(
    prompt="Modern minimalist architecture with clean lines",
    model_params={
        "workflow": archviz_workflow,
        "input_video": "clay_render.mp4",
        "style_image": "watercolor_reference.jpg"
    }
)
```

#### 1.2 Generative 2D In-betweening (Keyframe Interpolation)

**Use Case**: Animate hand-drawn architectural sketches with morphing transitions

**Option A: Basic Interpolation**
```json
{
  "workflow_type": "keyframe-interpolation-basic",
  "components": [
    "Wan VACE First Last Frame",
    "Frame Interpolation (FILM)",
    "VideoHelperSuite"
  ]
}
```

**Option B: Advanced ToonCrafter**
```json
{
  "workflow_type": "keyframe-interpolation-tooncrafter",
  "components": [
    "ToonCrafter (with Sparse Sketch Guidance)",
    "Toon Rectification Learning",
    "Dual-Reference 3D Decoder",
    "VideoHelperSuite"
  ],
  "required_custom_nodes": [
    "ComfyUI-ToonCrafter-Node"
  ]
}
```

**Implementation Priority**: Start with Option A (lower complexity), migrate to Option B as ToonCrafter matures.

#### 1.3 2.5D Parallax for Conceptual Sketches

**Use Case**: Create immersive parallax animations from single 2D drawings

**Required Workflow Nodes**:
```json
{
  "workflow_type": "2.5d-parallax",
  "components": [
    "MiDaS Depth Estimation",
    "ComfyUI-Depthflow-Nodes (motion control)",
    "Depth-of-Field Effects",
    "Camera Path Animation",
    "VideoHelperSuite"
  ],
  "required_custom_nodes": [
    "ComfyUI-Depthflow-Nodes"
  ]
}
```

**Parameters**:
- Parallax intensity (0.0 - 1.0)
- Camera motion path (linear, circular, custom)
- DOF focal plane
- Animation duration

**Database Schema Extension**:
```python
# Add to Workflow model (already supports arbitrary JSON)
workflow_json = {
    "type": "2.5d-parallax",
    "nodes": {...},
    "default_params": {
        "parallax_intensity": 0.5,
        "motion_path": "circular",
        "duration_seconds": 10
    }
}
```

---

### Phase 2: Advanced Features (Weeks 4-8)

#### 2.1 Prompt Scheduling (Dynamic Keyframing)

**Objective**: Enable time-based prompt transitions for evolving scenes

**Current Limitation**:
```python
# Current: Static prompt for entire generation
prompt = "A modern building at sunset"
```

**Enhanced Capability**:
```python
# Future: Keyframed prompts over time
prompt_schedule = [
    {"frame": 0, "prompt": "A modern building at dawn", "weight": 1.0},
    {"frame": 30, "prompt": "A modern building at midday", "weight": 1.0},
    {"frame": 60, "prompt": "A modern building at sunset", "weight": 1.0}
]
```

**Implementation Strategy**:

1. **Backend Schema Extension** (`backend/models/segment.py`):
```python
class Segment(Base, TimestampMixin):
    # ... existing fields ...

    # New: Support prompt scheduling
    prompt_schedule = Column(JSON, nullable=True)
    # Format: [{"frame": int, "prompt": str, "weight": float}]

    # Backward compatibility: If prompt_schedule is null, use static prompt
    @property
    def effective_prompt(self):
        return self.prompt_schedule if self.prompt_schedule else self.prompt
```

2. **Frontend Enhancement** (`frontend/src/features/timeline/SegmentCard.tsx`):
```typescript
interface PromptKeyframe {
  frame: number
  prompt: string
  weight: number
}

interface SegmentCardProps {
  // ... existing props ...
  promptSchedule?: PromptKeyframe[]
  onPromptScheduleChange?: (schedule: PromptKeyframe[]) => void
}

// Add keyframe timeline UI for advanced users
```

3. **ComfyUI Integration** (requires **ComfyUI_FizzNodes**):
```python
# workers/ai_worker/adapters/comfyui.py - Enhanced _inject_prompt method

def _inject_prompt_schedule(
    self,
    workflow: Dict[str, Any],
    prompt_schedule: List[Dict[str, Any]],
    model_params: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Inject keyframed prompts using FizzNodes Prompt Travel.

    Requires: ComfyUI_FizzNodes custom node pack
    """
    if not prompt_schedule or len(prompt_schedule) == 1:
        # Fallback to static prompt injection
        return self._inject_prompt(workflow, prompt_schedule[0]["prompt"], model_params)

    # Construct FizzNodes PromptTravel syntax
    # Format: "0: prompt_a | 30: prompt_b | 60: prompt_c"
    travel_syntax = " | ".join([
        f"{kf['frame']}: {kf['prompt']}"
        for kf in prompt_schedule
    ])

    # Inject into PromptTravel node (requires workflow template support)
    prompt_travel_node_id = model_params.get("prompt_travel_node_id", "10")
    if prompt_travel_node_id in workflow:
        workflow[prompt_travel_node_id]["inputs"]["text"] = travel_syntax

    return workflow
```

**Required Custom Nodes**:
- [ComfyUI_FizzNodes](https://github.com/FizzleDorf/ComfyUI_FizzNodes) - Prompt Travel/Scheduling

**UI/UX Considerations**:
- **Simple Mode**: Single prompt (existing behavior)
- **Advanced Mode**: Timeline with keyframe markers for prompt editing
- Toggle in Timeline Editor settings

#### 2.2 ControlNet Integration for Structural Consistency

**Objective**: Preserve architectural geometry during stylistic transformations

**Use Case**: Convert photorealistic 3D render ‚Üí stylized sketch while maintaining building structure

**Current Workflow Gap**:
```python
# Current: No structural guidance
model_params = {
    "workflow": basic_workflow,
    "prompt": "watercolor architectural sketch"
}
# Result: May lose geometric accuracy
```

**Enhanced Workflow**:
```python
# Future: ControlNet ensures geometry preservation
model_params = {
    "workflow": controlnet_archviz_workflow,
    "prompt": "watercolor architectural sketch",
    "control_inputs": {
        "lineart": "building_lineart.png",  # Extracted edges
        "depth": "building_depth.png"        # Depth map
    }
}
# Result: Stylized while preserving structure
```

**Implementation**:

1. **New Workflow Template** (`workflows/archviz-controlnet-style-transfer.json`):
```json
{
  "nodes": {
    "1": {"class_type": "LoadImage", "inputs": {"image": "input.png"}},
    "2": {"class_type": "ControlNetApply", "inputs": {
      "conditioning": ["7", 0],
      "control_net": ["8", 0],
      "image": ["3", 0],  // Lineart preprocessor
      "strength": 0.85
    }},
    "3": {"class_type": "LineartPreprocessor", "inputs": {"image": ["1", 0]}},
    "4": {"class_type": "DepthPreprocessor", "inputs": {"image": ["1", 0]}},
    "5": {"class_type": "ControlNetApply", "inputs": {
      "conditioning": ["2", 0],
      "control_net": ["9", 0],
      "image": ["4", 0],  // Depth map
      "strength": 0.70
    }},
    "6": {"class_type": "CLIPTextEncode", "inputs": {"text": "watercolor sketch"}},
    "7": {"class_type": "CLIPTextEncode", "inputs": {"text": "negative"}},
    "8": {"class_type": "ControlNetLoader", "inputs": {"control_net_name": "control_sd15_lineart.pth"}},
    "9": {"class_type": "ControlNetLoader", "inputs": {"control_net_name": "control_sd15_depth.pth"}}
  }
}
```

2. **Model Downloads** (document in setup guide):
```bash
# ComfyUI/models/controlnet/
wget https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_lineart.pth
wget https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1p_sd15_depth.pth
```

3. **Required Custom Nodes**:
- [comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) - Preprocessors

**No Backend Code Changes Required** - fully supported by existing workflow API

#### 2.3 IPAdapter for Style Consistency

**Objective**: Apply reference image style to generated content

**Use Case**: "Make this ArchViz render look like this artist's watercolor style"

**Implementation** (`workflows/archviz-style-transfer-ipadapter.json`):

```json
{
  "workflow_type": "style-transfer-ipadapter",
  "description": "Apply reference image style to architectural renders",
  "nodes": {
    "ip_adapter_node": {
      "class_type": "IPAdapter",
      "inputs": {
        "model": ["checkpoint_loader", 0],
        "image": "style_reference.jpg",
        "weight": 0.8,
        "weight_type": "original"
      }
    }
  }
}
```

**Required Models**:
- `ip-adapter_sd15.safetensors` or `ip-adapter_sdxl.safetensors`
- Place in: `ComfyUI/models/ipadapter/`

**Required Custom Nodes**:
- [ComfyUI_IPAdapter_plus](https://github.com/cubiq/ComfyUI_IPAdapter_plus)

**Frontend Integration** (Timeline Editor):
```typescript
// Add style reference image uploader to segment form
<input
  type="file"
  accept="image/*"
  onChange={(e) => setStyleReference(e.target.files[0])}
/>

// Include in model_params
model_params: {
  workflow: ipadapter_workflow,
  style_reference_url: uploadedStyleImageUrl
}
```

---

### Phase 3: Production Integration (Weeks 9-16)

#### 3.1 Blender Plugin (Optional - High Value)

**Objective**: Enable direct export from Blender ‚Üí Video Foundry workflow

**Reference Implementation**: [ComfyUI-BlenderAI-node](https://github.com/AIGODLIKE/ComfyUI-BlenderAI-node)

**Architecture**:
```
Blender (Artist Workspace)
    ‚Üì (Python API Plugin)
Video Foundry Backend API
    ‚Üì (RabbitMQ)
AI Workers (ComfyUI)
    ‚Üì
Final Render
```

**Plugin Capabilities**:
1. Export Grease Pencil 2D animations as keyframes
2. Export 3D renders + depth maps
3. Send to Video Foundry API
4. Monitor render progress in Blender viewport

**Implementation Scope**:
- Blender Add-on (Python): ~800 lines
- Backend API: **No changes needed** - already supports REST API
- Documentation: Setup guide for artists

**Distribution**:
```bash
# Blender add-on structure
video-foundry-blender-plugin/
‚îú‚îÄ‚îÄ __init__.py              # Blender registration
‚îú‚îÄ‚îÄ operators.py             # Export operators
‚îú‚îÄ‚îÄ panels.py                # UI panels
‚îú‚îÄ‚îÄ api_client.py            # Video Foundry REST client
‚îî‚îÄ‚îÄ README.md
```

#### 3.2 Enhanced Node-Based UI (Optional - Advanced Users)

**Objective**: Provide React Flow-based visual workflow editor for power users

**Current State**: Timeline-based UI (artist-friendly)
**Enhancement**: Optional node editor for advanced workflow customization

**Technology Stack**:
- [React Flow](https://reactflow.dev/) - Node graph library
- Use [AI Workflow Editor template](https://reactflow.dev/examples/ai-workflow-editor)

**Architecture**:
```typescript
// frontend/src/features/workflow-editor/ (NEW)

interface NodeEditorProps {
  workflowJson: ComfyUIWorkflow
  onWorkflowChange: (workflow: ComfyUIWorkflow) => void
}

// Two UI modes:
// 1. Timeline Mode (existing) - Simple, timeline-based
// 2. Node Mode (new) - Advanced, graph-based

// Toggle in settings
const [uiMode, setUIMode] = useState<'timeline' | 'nodes'>('timeline')
```

**Implementation Priority**: Low (Phase 3) - existing timeline UI serves 90% of users

**Value Proposition**: Enables advanced users to create custom workflows without leaving the web UI

---

## Technical Implementation Guide

### Quick Start: Adding Your First ArchViz Workflow

**Step 1: Create Workflow in ComfyUI**

1. Open ComfyUI web interface (http://localhost:8188)
2. Build workflow: AnimateDiff + ControlNet Lineart + IPAdapter
3. Enable "Dev mode Options" in Settings
4. Export as "Save (API Format)" ‚Üí `archviz-stylized-v1.json`

**Step 2: Add to Video Foundry**

```bash
# Copy workflow to templates directory
cp archviz-stylized-v1.json /home/user/Jynco/workflows/

# Test via Python API
cd /home/user/Jynco
python test_workflow.py workflows/archviz-stylized-v1.json
```

**Step 3: Register in Database**

```python
# backend/scripts/seed_archviz_workflows.py
import json
from backend.models.workflow import Workflow
from backend.config.database import SessionLocal

def seed_archviz_workflows():
    db = SessionLocal()

    with open('workflows/archviz-stylized-v1.json') as f:
        workflow_json = json.load(f)

    workflow = Workflow(
        name="Stylized ArchViz (Video-to-Video)",
        description="Transform 3D renders into stylized 2D animations",
        category="archviz",
        workflow_json=workflow_json,
        is_default=True,
        user_id=system_user_id  # System workflows
    )

    db.add(workflow)
    db.commit()

if __name__ == "__main__":
    seed_archviz_workflows()
```

**Step 4: Use in Frontend**

```typescript
// Already supported - no code changes needed!
const { data: workflows } = useQuery({
  queryKey: ['workflows', { category: 'archviz' }],
  queryFn: () => workflowsApi.listWorkflows({ category: 'archviz' })
})

// Workflows appear in Timeline Editor dropdown automatically
```

### Required ComfyUI Custom Node Packs

Update `docs/COMFYUI_SETUP_GUIDE.md` with:

```bash
# Navigate to ComfyUI custom_nodes directory
cd ComfyUI/custom_nodes

# Core (already documented)
git clone https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved.git
git clone https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite.git

# NEW: ArchViz Enhancement Pack
git clone https://github.com/Fannovel16/comfyui_controlnet_aux.git        # ControlNet preprocessors
git clone https://github.com/cubiq/ComfyUI_IPAdapter_plus.git             # Style transfer
git clone https://github.com/FizzleDorf/ComfyUI_FizzNodes.git             # Prompt scheduling
git clone https://github.com/BlenderNeko/ComfyUI_Noise.git                # Advanced noise control
git clone https://github.com/banodoco/Steerable-Motion.git                # Motion control

# OPTIONAL: Advanced Features
git clone https://github.com/FuouM/ComfyUI-Depthflow-Nodes.git           # 2.5D Parallax
git clone https://github.com/kijai/ComfyUI-ToonCrafter.git               # Cartoon interpolation (when available)

# Restart ComfyUI
cd ../..
python main.py --listen --port 8188
```

### Model Downloads

**ControlNet Models** (~1.5GB each):
```bash
cd ComfyUI/models/controlnet
wget https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_lineart.pth
wget https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1p_sd15_depth.pth
wget https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_canny.pth
```

**IPAdapter Models** (~100MB):
```bash
cd ComfyUI/models/ipadapter
wget https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.safetensors
wget https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl_vit-h.safetensors
```

---

## Workflow Templates Specification

### Template 1: Stylized ArchViz Walkthrough

**Filename**: `workflows/archviz-video-to-video-stylized.json`

**Inputs**:
- `input_video`: Path to 3D render video (clay/grayscale)
- `prompt`: Artistic style description
- `style_reference` (optional): Reference image for IPAdapter

**Outputs**:
- Stylized MP4 video (same duration as input)

**Parameters**:
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `controlnet_lineart_strength` | float | 0.85 | How strictly to preserve line art (0.0-1.0) |
| `controlnet_depth_strength` | float | 0.70 | How strictly to preserve depth (0.0-1.0) |
| `ipadapter_weight` | float | 0.75 | Style reference influence (0.0-1.0) |
| `cfg_scale` | float | 7.5 | Prompt adherence (1.0-20.0) |
| `steps` | int | 25 | Sampling steps (quality vs speed) |

**Use Cases**:
- Transform Blender architectural renders into watercolor paintings
- Convert SketchUp exports into manga-style illustrations
- Apply artistic styles to photorealistic walkthroughs

### Template 2: Keyframe Interpolation (Basic)

**Filename**: `workflows/archviz-keyframe-interpolation-basic.json`

**Inputs**:
- `first_frame`: Starting keyframe image
- `last_frame`: Ending keyframe image
- `frame_count`: Number of in-between frames to generate

**Outputs**:
- Interpolated video showing smooth transition

**Parameters**:
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `interpolation_frames` | int | 30 | Number of generated frames |
| `fps` | int | 12 | Output frame rate |
| `smoothing` | float | 0.5 | Motion smoothing (0.0-1.0) |

**Use Cases**:
- Animate between two architectural sketch concepts
- Show building design evolution over time
- Create "morphing" transitions between floor plans

### Template 3: 2.5D Parallax Animation

**Filename**: `workflows/archviz-2.5d-parallax.json`

**Inputs**:
- `input_image`: Single 2D architectural drawing/sketch
- `depth_map` (optional): Custom depth map (auto-generated if not provided)

**Outputs**:
- Parallax animation video with camera motion

**Parameters**:
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `parallax_intensity` | float | 0.5 | Depth separation strength (0.0-1.0) |
| `motion_type` | string | "circular" | Camera path: "linear", "circular", "zoom" |
| `duration_seconds` | int | 10 | Animation length |
| `dof_strength` | float | 0.3 | Depth-of-field blur (0.0-1.0) |
| `fps` | int | 30 | Output frame rate |

**Use Cases**:
- Add dimension to concept sketches for presentations
- Create immersive social media content from single images
- Enhance static renders with subtle motion

---

## Database Schema Updates

### Workflow Categories Expansion

**Current Categories** (implied by existing data):
- `text-to-video`
- `image-to-video`

**New Categories** (add to seed data):
```python
# backend/migrations/add_archviz_categories.py

ARCHVIZ_CATEGORIES = [
    "archviz",                    # General architectural visualization
    "archviz-style-transfer",     # ControlNet + IPAdapter workflows
    "archviz-interpolation",      # Keyframe in-betweening
    "archviz-parallax",           # 2.5D depth-based animation
    "archviz-render-enhancement", # Enhance existing renders
]
```

**API Update** (already supported):
```bash
GET /api/workflows?category=archviz
GET /api/workflows?category=archviz-style-transfer
```

### Segment Model Enhancement (Optional)

**Add prompt scheduling support**:

```python
# backend/models/segment.py

class Segment(Base, TimestampMixin):
    # ... existing fields ...

    # NEW: Optional prompt keyframing
    prompt_schedule = Column(JSON, nullable=True)
    # Format: [{"frame": 0, "prompt": "...", "weight": 1.0}, ...]

    # Backward compatibility
    def get_effective_prompt(self):
        if self.prompt_schedule:
            return self.prompt_schedule
        return self.prompt  # Legacy static prompt
```

**Migration**:
```bash
cd backend
alembic revision --autogenerate -m "Add prompt_schedule to segments"
alembic upgrade head
```

---

## Testing Strategy

### Phase 1: Workflow Validation

**Test each new workflow template**:

```bash
# Use existing test utility
cd /home/user/Jynco
python test_workflow.py workflows/archviz-video-to-video-stylized.json

# Verify output
ls -lh /path/to/comfyui/output/
```

**Automated Testing** (expand existing test suite):

```python
# tests/integration/test_archviz_workflows.py

import pytest
from workers.ai_worker.adapters.comfyui import ComfyUIAdapter

@pytest.mark.integration
@pytest.mark.slow
async def test_archviz_stylized_workflow():
    """Test stylized ArchViz video-to-video workflow."""
    adapter = ComfyUIAdapter(config={"comfyui_url": "http://localhost:8188"})

    with open('workflows/archviz-video-to-video-stylized.json') as f:
        workflow = json.load(f)

    job_id = await adapter.initiate_generation(
        prompt="Modern architecture in watercolor style",
        model_params={
            "workflow": workflow,
            "input_video": "tests/fixtures/test_render.mp4"
        }
    )

    # Poll for completion (timeout 5 minutes)
    result = await wait_for_completion(adapter, job_id, timeout=300)

    assert result.status == GenerationStatus.COMPLETED
    assert result.video_url is not None
    # Optional: Validate video format, duration, resolution

@pytest.mark.integration
async def test_prompt_scheduling():
    """Test keyframed prompt transitions."""
    # ... implementation
```

### Phase 2: E2E User Workflows

**Test Scenarios**:

1. **Upload 3D Render ‚Üí Apply Style ‚Üí Download**
   - User uploads MP4 from Blender
   - Selects "Stylized ArchViz" workflow
   - Chooses watercolor style reference
   - Generates and downloads result

2. **Keyframe Animation ‚Üí Interpolation**
   - User uploads two architectural sketches
   - Uses interpolation workflow
   - Generates smooth 60-frame transition

3. **Single Sketch ‚Üí Parallax Animation**
   - User uploads single concept drawing
   - Applies 2.5D parallax workflow
   - Generates 10-second camera motion

**Test Execution**:
```bash
# Run full E2E test suite
cd tests
pytest -v -m e2e tests/e2e/test_archviz_workflows.py
```

---

## Documentation Updates

### User-Facing Documentation

**1. Create New Guide**: `docs/ARCHVIZ_GUIDE.md`

```markdown
# Architectural Visualization with Video Foundry

## Introduction
Video Foundry offers specialized workflows for architects, designers, and ArchViz professionals...

## Workflows

### Stylized Render Transformation
Turn your 3D renders into artistic styles...

### Sketch Animation
Bring hand-drawn concepts to life...

### Parallax Enhancement
Add depth and motion to static drawings...

## Tutorials
- [Tutorial 1: Watercolor ArchViz from Blender]
- [Tutorial 2: Animated Floor Plan Transitions]
- [Tutorial 3: Conceptual Sketch to Video]
```

**2. Update Existing Docs**:

- `QUICKSTART.md` - Add ArchViz quick start section
- `USER_GUIDE.md` - New chapter on ArchViz workflows
- `COMFYUI_SETUP_GUIDE.md` - Document new custom nodes
- `WORKFLOW_LIBRARY_GUIDE.md` - Document new workflow categories

### Developer Documentation

**1. Update `CLAUDE.md`**:

```markdown
## ArchViz Integration

Video Foundry includes specialized workflows for architectural visualization:

### Workflow Types
- **archviz-style-transfer**: ControlNet + IPAdapter for stylized renders
- **archviz-interpolation**: Keyframe animation
- **archviz-parallax**: 2.5D depth-based motion

### Adding New ArchViz Workflows
[Step-by-step guide]
```

**2. Update API Documentation** (`QUICK_REFERENCE.md`):

```markdown
### Workflow API - ArchViz Categories

GET /api/workflows?category=archviz
GET /api/workflows?category=archviz-style-transfer
...
```

---

## Marketing & Positioning

### Unique Value Proposition

**Before**: "AI video generation platform"
**After**: "The only AI video platform purpose-built for architectural visualization"

### Target Audience Expansion

**Current**: General AI video creators
**New Primary**: Architects, ArchViz studios, design agencies
**New Secondary**: Concept artists, game environment designers

### Feature Highlights

1. **Preserve Your Geometry**: ControlNet ensures architectural accuracy
2. **Artist-Controlled**: Keyframe-based workflow, not black-box generation
3. **Stylistic Freedom**: Transform renders into any artistic style
4. **Production-Ready**: Blender integration for professional workflows
5. **Open Ecosystem**: No vendor lock-in, use any ComfyUI workflow

### Competitive Differentiation

| Platform | ArchViz Support | Geometry Preservation | Artist Control | Open Source |
|----------|----------------|---------------------|----------------|-------------|
| **Video Foundry** | ‚úÖ Specialized | ‚úÖ ControlNet | ‚úÖ Timeline + Keyframes | ‚úÖ Full |
| Runway Gen-3 | ‚ùå Generic | ‚ùå Black box | ‚ö†Ô∏è Prompt only | ‚ùå Closed |
| Stability AI | ‚ö†Ô∏è Partial | ‚ùå Limited | ‚ö†Ô∏è API only | ‚ö†Ô∏è Models only |
| Pika Labs | ‚ùå Generic | ‚ùå Black box | ‚ö†Ô∏è Prompt only | ‚ùå Closed |

---

## Resource Requirements

### Compute Resources

**ComfyUI Server** (for optimal ArchViz performance):

| Component | Minimum | Recommended | Professional |
|-----------|---------|-------------|--------------|
| GPU | NVIDIA RTX 3060 (12GB) | RTX 4070 (12GB) | RTX 4090 (24GB) |
| RAM | 16GB | 32GB | 64GB |
| Storage | 100GB SSD | 500GB NVMe | 1TB NVMe |
| Model Storage | ~50GB | ~100GB | ~200GB |

**Scaling**:
- Basic: 1 worker, handles 1 concurrent job
- Standard: 2-4 workers, handles 2-4 concurrent jobs
- Professional: 8+ workers with load balancing

### Model Storage Requirements

**Base Models** (~20GB):
- SDXL: 6.9GB
- SD 1.5: 4GB
- AnimateDiff: 1.7GB
- SVD: 7GB

**ControlNet Models** (~6GB):
- Lineart: 1.5GB
- Depth: 1.5GB
- Canny: 1.5GB
- Others: ~1.5GB each

**IPAdapter Models** (~500MB):
- SD 1.5: 100MB
- SDXL: 400MB

**Total for Full ArchViz Setup**: ~80-100GB

---

## Success Metrics

### Phase 1 KPIs (Weeks 1-3)

- [ ] 3 new ArchViz workflow templates deployed
- [ ] 100% backward compatibility maintained
- [ ] < 5% increase in average render time
- [ ] Documentation complete for all new workflows

### Phase 2 KPIs (Weeks 4-8)

- [ ] Prompt scheduling functional with FizzNodes
- [ ] ControlNet workflows tested on 10+ real ArchViz projects
- [ ] User feedback collected from 5+ architecture firms
- [ ] E2E test coverage > 80%

### Phase 3 KPIs (Weeks 9-16)

- [ ] Blender plugin beta released
- [ ] Optional React Flow UI prototype complete
- [ ] 20+ ArchViz-specific workflow templates in library
- [ ] 50%+ of new users selecting ArchViz workflows

### Adoption Metrics

**Track via analytics**:
- Workflow category usage distribution
- ArchViz workflow popularity ranking
- Average render quality (user ratings)
- ControlNet vs. non-ControlNet result comparison
- User retention for ArchViz-focused users vs. general users

---

## Risk Assessment & Mitigation

### Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **ComfyUI Custom Node Instability** | Medium | High | Pin node versions, maintain fallbacks, extensive testing |
| **ControlNet Model Compatibility** | Low | Medium | Document supported models, provide pre-tested downloads |
| **Prompt Scheduling Complexity** | Medium | Medium | Offer simple mode (static) + advanced mode (keyframed) |
| **Increased VRAM Requirements** | High | Medium | Clear documentation, model optimization guides, LowVRAM mode |
| **Workflow Template Fragility** | Medium | High | Version all templates, automated validation tests |

### Operational Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **User Confusion (Too Complex)** | High | High | Excellent documentation, video tutorials, preset workflows |
| **Support Burden (New Features)** | Medium | Medium | Comprehensive troubleshooting docs, community forum |
| **Model Download Barriers** | Medium | Low | Automated setup script, pre-configured Docker image |

### Strategic Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Niche Market Too Small** | Low | Medium | Maintain general-purpose capabilities alongside ArchViz |
| **Proprietary Competitors** | Medium | Medium | Open-source moat, faster innovation cycles |
| **ToonCrafter Adoption Delay** | High | Low | Use proven interpolation methods first, ToonCrafter as upgrade path |

---

## Future Roadmap (Beyond Phase 3)

### Advanced Features (6-12 Months)

1. **Physics-Guided Animation** (when available)
   - Integrate physics-aware models (e.g., PhysAnimator)
   - Realistic building deformations, wind effects
   - Gravity-aware object motion

2. **Unified Generative Pipeline**
   - Single model: Storyboard ‚Üí Layout ‚Üí Keyframes ‚Üí In-betweening
   - End-to-end ArchViz story generation from text description

3. **AI Agent Integration**
   - "AI-as-agent" suggesting workflow optimizations
   - Automated A/B testing of stylistic variations
   - Intelligent workflow construction from natural language

4. **Multi-Modal Input**
   - Voice-to-video: Describe architecture verbally ‚Üí generate animation
   - 3D model direct import: FBX/OBJ ‚Üí stylized video
   - Point cloud ‚Üí ArchViz animation pipeline

### Platform Evolution

1. **Cloud-Hosted Option**
   - Managed ComfyUI service (remove local setup burden)
   - GPU time marketplace
   - Pre-loaded all ArchViz models

2. **Collaboration Features**
   - Multi-user project editing
   - Version control for workflows
   - Team libraries of style references

3. **Marketplace**
   - Community-contributed ArchViz workflows
   - Premium style packs
   - Professional training datasets

---

## Conclusion

Video Foundry's existing architecture aligns remarkably well with the recommended "New Blueprint" for AI-driven 2D animation pipelines. The platform's **modular, open-source-first design** provides a strong foundation for integrating advanced ArchViz capabilities with minimal technical debt.

### Key Takeaways

1. **80% Already Built**: Core infrastructure (ComfyUI integration, workflow library, timeline UI) is production-ready
2. **20% Quick Wins**: Adding ControlNet, IPAdapter, and prompt scheduling workflows requires no backend changes
3. **Strategic Differentiation**: ArchViz specialization creates a defensible market position
4. **Scalable Implementation**: Phased roadmap balances quick value delivery with long-term innovation

### Immediate Next Steps

**Week 1**:
- [ ] Install ControlNet custom nodes and models
- [ ] Create first ArchViz workflow template (Style Transfer)
- [ ] Test with real architectural render

**Week 2**:
- [ ] Create keyframe interpolation workflow
- [ ] Document setup process for ArchViz features
- [ ] Begin user testing with architecture students/firms

**Week 3**:
- [ ] Create 2.5D parallax workflow
- [ ] Add ArchViz category to workflow library
- [ ] Publish initial ArchViz guide documentation

---

**Document Maintainer**: Video Foundry Architecture Team
**Last Updated**: November 14, 2025
**Version**: 1.0
**Status**: Strategic Planning Document
**Next Review**: December 2025
